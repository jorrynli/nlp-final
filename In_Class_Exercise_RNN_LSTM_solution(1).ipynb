{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **PAIR ASSIGNMENT**\n",
        "\n",
        "Please work in pairs for this exercise. Pair up with the person next to you. If you find that there isn't anyone sitting next to you or if you're unable to form a pair, please raise your hand, and I will assist in pairing you with someone.\n",
        "\n",
        "\n",
        "#### **TODO: Please assign your full name and your partner's full name to the variables below, respectively.**\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "your_name = 'Fatma Tarlaci'\n",
        "your_partner_name = 'Sagnik Majumder'\n",
        "```"
      ],
      "metadata": {
        "id": "LsjSprMALgOq"
      },
      "id": "LsjSprMALgOq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign your and your partner's names here.\n",
        "# If you find that there isn't anyone sitting next to you\n",
        "# or if you're unable to form a pair, please raise your hand.\n",
        "# If, in the end, we are unable to find a partner for you,\n",
        "# please assign the word \"self\" to the `your_partner_name` variable.\n",
        "your_name = ''\n",
        "your_partner_name = ''"
      ],
      "metadata": {
        "id": "pX9WIdxTLiL1"
      },
      "id": "pX9WIdxTLiL1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eb782420",
      "metadata": {
        "id": "eb782420"
      },
      "source": [
        "\n",
        "# **Vanishing Gradients, RNN vs LSTM (Language Modeling)**\n",
        "\n",
        "**Goals**\n",
        "- Observe **vanishing gradients** in a vanilla RNN language model.\n",
        "- Implement and compare a stronger model (**LSTM**) on the same data.\n",
        "- Measure **perplexity** (PP) and **gradient norms** to see the difference.\n",
        "\n",
        "> **Time box:** ~45–60 minutes. We work on a **tiny** subset of WikiText-2 for speed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b413c1",
      "metadata": {
        "id": "83b413c1"
      },
      "source": [
        "\n",
        "## 0) Setup\n",
        "\n",
        "Run the following to install dependencies (if needed) and import libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b324f489",
      "metadata": {
        "id": "b324f489"
      },
      "outputs": [],
      "source": [
        "# If running in Colab or a fresh environment, uncomment:\n",
        "# !pip -q install torch datasets tqdm\n",
        "\n",
        "import math\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Iterable, Optional\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b8c7af",
      "metadata": {
        "id": "83b8c7af"
      },
      "outputs": [],
      "source": [
        "# Reproducibility and device selection\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af218c43",
      "metadata": {
        "id": "af218c43"
      },
      "source": [
        "\n",
        "## **TASK 1: Observe the Data: Tiny WikiText-2 slice and the Vocabulary implementation**\n",
        "\n",
        "We will use **Hugging Face** `wikitext` (`wikitext-2-raw-v1`) word-level modeling. You can see its details here: https://huggingface.co/datasets/Salesforce/wikitext/viewer/wikitext-2-raw-v1\n",
        "\n",
        "**Review and discuss the Pipeline implemented below. Pipeline**\n",
        "1. Loads the training and validation splits.\n",
        "2. Builds a word vocabulary from a **small slice** of the training split.\n",
        "3. Converts text to IDs and create fixed-length sequences for language modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be1c189",
      "metadata": {
        "id": "2be1c189"
      },
      "outputs": [],
      "source": [
        "def build_vocab(texts: Iterable[str], max_tokens: int = 20000) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "    \"\"\"Build a simple word-index vocabulary.\n",
        "\n",
        "    Args:\n",
        "        texts: Iterable of documents (strings) used to build the vocabulary.\n",
        "        max_tokens: Maximum vocab size including special tokens.\n",
        "\n",
        "    Returns:\n",
        "        A pair (stoi, itos) where:\n",
        "            stoi: dict mapping string tokens to integer IDs.\n",
        "            itos: dict mapping IDs back to string tokens.\n",
        "    \"\"\"\n",
        "    # Special tokens\n",
        "    specials = [\"<pad>\", \"<unk>\"]\n",
        "    freq: Dict[str, int] = {}\n",
        "    for doc in texts:\n",
        "        # Lowercase and split by whitespace for simplicity\n",
        "        for w in doc.strip().lower().split():\n",
        "            freq[w] = freq.get(w, 0) + 1\n",
        "    # Sort by frequency (desc), then alphabetically to stabilize ties\n",
        "    items = sorted(freq.items(), key=lambda x: (-x[1], x[0]))\n",
        "    # Truncate to max_tokens - len(specials)\n",
        "    trimmed = items[: max(0, max_tokens - len(specials))]\n",
        "    # Build vocab dicts\n",
        "    itos = {0: \"<pad>\", 1: \"<unk>\"}\n",
        "    for i, (w, _) in enumerate(trimmed, start=len(specials)):\n",
        "        itos[i] = w\n",
        "    stoi = {w: i for i, w in itos.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "\n",
        "def encode(text: str, stoi: Dict[str, int]) -> List[int]:\n",
        "    \"\"\"Encode a string into a list of token IDs using the provided vocab.\n",
        "\n",
        "    Args:\n",
        "        text: Input string (document or sentence).\n",
        "        stoi: Vocabulary mapping from token string to index.\n",
        "\n",
        "    Returns:\n",
        "        List of integer token IDs. Unknown tokens map to <unk>.\n",
        "    \"\"\"\n",
        "    unk = stoi.get(\"<unk>\", 1)\n",
        "    return [stoi.get(w, unk) for w in text.strip().lower().split()]\n",
        "\n",
        "\n",
        "class LMDataset(Dataset):\n",
        "    \"\"\"A tiny language modeling dataset built from text token IDs.\n",
        "\n",
        "    Each example is a pair (x, y) where:\n",
        "      - x is a sequence of token IDs of length `seq_len`\n",
        "      - y is the next-token targets of length `seq_len` (i.e., x shifted by one)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ids: List[int], seq_len: int = 30):\n",
        "        \"\"\"Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            ids: A long list of token IDs concatenated from documents.\n",
        "            seq_len: Unroll length for truncated BPTT.\n",
        "        \"\"\"\n",
        "        self.ids = ids\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        # Number of full sequences we can make (minus 1 for the next-token target)\n",
        "        return max(0, len(self.ids) - self.seq_len - 1)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Compute the starting index of the subsequence\n",
        "        start = idx\n",
        "\n",
        "        # Input sequence: grab tokens from [start : start + seq_len]\n",
        "        x = self.ids[start : start + self.seq_len]\n",
        "\n",
        "        # Target sequence: same as input but shifted one step to the right\n",
        "        y = self.ids[start + 1 : start + self.seq_len + 1]\n",
        "\n",
        "        # Return both sequences as PyTorch tensors of type long (integer indices)\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "\n",
        "# 1) Load small wikitext-2-raw-v1 dataset\n",
        "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "# 2) Take a tiny slice for speed (e.g., first 10k chars worth of tokens)\n",
        "train_texts = ds[\"train\"][\"text\"][:2000]   # small slice of documents\n",
        "valid_texts = ds[\"validation\"][\"text\"][:500]\n",
        "\n",
        "# 3) Build vocab from the training\n",
        "# build_vocab(...) takes the training texts and constructs a mapping from string tokens → integer IDs.\n",
        "# stoi: string-to-index dictionary (e.g., \"the\" → 5).\n",
        "# itos: index-to-string list (e.g., 5 → \"the\").\n",
        "# max_tokens=15000 means the vocabulary will be capped at 15,000 unique tokens.\n",
        "# The most frequent tokens are kept, and rarer ones usually get mapped to an <unk> token.\n",
        "stoi, itos = build_vocab(train_texts, max_tokens=15000)\n",
        "\n",
        "# You’ll use this vocab_size to define the embedding layer and the output layer of your language model.\n",
        "vocab_size = len(stoi)\n",
        "print(f'The number of unique tokens in your vocabulary (≤ 15,000) is:{vocab_size}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9941c016",
      "metadata": {
        "id": "9941c016"
      },
      "outputs": [],
      "source": [
        "# 4) Encode text into token IDs and concatenate\n",
        "train_ids: List[int] = []\n",
        "for t in train_texts:\n",
        "    train_ids.extend(encode(t, stoi))\n",
        "\n",
        "valid_ids: List[int] = []\n",
        "for t in valid_texts:\n",
        "    valid_ids.extend(encode(t, stoi))\n",
        "\n",
        "len(train_ids), len(valid_ids), vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89488c02",
      "metadata": {
        "id": "89488c02"
      },
      "outputs": [],
      "source": [
        "# 5) Build datasets and loaders\n",
        "SEQ_LEN = 30\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_ds = LMDataset(train_ids, seq_len=SEQ_LEN)\n",
        "valid_ds = LMDataset(valid_ids, seq_len=SEQ_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "len(train_ds), len(valid_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e354f0ad",
      "metadata": {
        "id": "e354f0ad"
      },
      "source": [
        "\n",
        "## **TASK 2: Implement Models**\n",
        "\n",
        "You will now implement and compare a **Vanilla RNN LM** and an **LSTM LM**. Both are small and trained briefly. Complete the models below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef0b09c",
      "metadata": {
        "id": "1ef0b09c"
      },
      "outputs": [],
      "source": [
        "class VanillaRNNLM(nn.Module):\n",
        "    \"\"\"A tiny Vanilla RNN language model.\n",
        "\n",
        "    Architecture:\n",
        "      - Embedding\n",
        "      - Manual loop with `nn.RNNCell` (single layer)\n",
        "      - Linear decoder to vocab logits\n",
        "\n",
        "    Notes:\n",
        "      We keep it tiny and explicit to make gradient tracking easy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int = 128, hidden_dim: int = 256, pad_idx: int = 0):\n",
        "      super().__init__()\n",
        "      # Initialize parent nn.Module (needed for all PyTorch models)\n",
        "\n",
        "      # Embedding layer: maps token IDs to dense vectors\n",
        "      # vocab_size = number of unique tokens in the vocabulary\n",
        "      # emb_dim = dimensionality of each embedding vector\n",
        "      # padding_idx ensures the pad token always maps to a zero vector\n",
        "      self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "\n",
        "      # RNNCell: processes one time step at a time\n",
        "      # Takes embedding input of size emb_dim, outputs hidden state of size hidden_dim\n",
        "      # Uses tanh nonlinearity by default\n",
        "      self.rnn_cell = nn.RNNCell(emb_dim, hidden_dim, nonlinearity=\"tanh\")\n",
        "\n",
        "      # Fully connected (linear) layer: projects hidden state → vocabulary logits\n",
        "      # Needed for predicting next-token probabilities\n",
        "      self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "      # Store hidden dimension size for later use (e.g., when initializing hidden state)\n",
        "      self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor, h0: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass through a sequence.\n",
        "\n",
        "        Args:\n",
        "            x: LongTensor of shape (batch, seq_len) with token IDs.\n",
        "            h0: Optional initial hidden state of shape (batch, hidden_dim).\n",
        "\n",
        "        Returns:\n",
        "            logits: FloatTensor of shape (batch, seq_len, vocab_size)\n",
        "            h: Final hidden state (batch, hidden_dim)\n",
        "        \"\"\"\n",
        "        batch, seqlen = x.size()\n",
        "        if h0 is None:\n",
        "            # We use x.new_zeros(...) instead of torch.zeros(..., device=x.device)\n",
        "            # because new_zeros automatically matches BOTH the device (CPU/GPU)\n",
        "            # and the dtype of the input tensor `x`.\n",
        "            # This trick avoids bugs where you accidentally create a hidden state\n",
        "            # on the wrong device or with the wrong precision, especially in Colab\n",
        "            # when switching between CPU and GPU. This also helps avoid a common\n",
        "            # beginner pitfall: mismatched devices (RuntimeError: Expected all tensors to be on the same device).\n",
        "            h = x.new_zeros((batch, self.hidden_dim), dtype=torch.float32)\n",
        "        else:\n",
        "            h = h0\n",
        "\n",
        "        # Prepare a list to collect per-step logits\n",
        "        logits_steps = []\n",
        "        for t in range(seqlen):\n",
        "            # Embed current time-step tokens\n",
        "            emb_t = self.emb(x[:, t])\n",
        "            # One RNNCell step\n",
        "            h = self.rnn_cell(emb_t, h)\n",
        "            # Decode to vocabulary logits\n",
        "            logits_t = self.fc(h)\n",
        "            logits_steps.append(logits_t.unsqueeze(1))  # keep time dimension\n",
        "\n",
        "        # Concatenate over time → (batch, seq_len, vocab_size)\n",
        "        logits = torch.cat(logits_steps, dim=1)\n",
        "        return logits, h\n",
        "\n",
        "\n",
        "class LSTMLM(nn.Module):\n",
        "    \"\"\"A tiny LSTM language model using `nn.LSTM`.\n",
        "\n",
        "    Architecture:\n",
        "      - Embedding\n",
        "      - Single-layer LSTM\n",
        "      - Linear decoder to vocab logits\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int = 128, hidden_dim: int = 256, pad_idx: int = 0):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, state=None) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"Forward pass through a sequence.\n",
        "\n",
        "        Args:\n",
        "            x: LongTensor of shape (batch, seq_len) with token IDs.\n",
        "            state: Optional tuple (h0, c0) initial states of shape (1, batch, hidden_dim).\n",
        "\n",
        "        Returns:\n",
        "            logits: FloatTensor of shape (batch, seq_len, vocab_size)\n",
        "            new_state: Tuple (h, c) final states.\n",
        "        \"\"\"\n",
        "        emb = self.emb(x)                       # (batch, seq_len, emb_dim)\n",
        "        out, new_state = self.lstm(emb, state)  # (batch, seq_len, hidden_dim)\n",
        "        logits = self.fc(out)                   # (batch, seq_len, vocab_size)\n",
        "        return logits, new_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c7c677",
      "metadata": {
        "id": "d7c7c677"
      },
      "source": [
        "\n",
        "## **TASK 3: Loss, Perplexity, and Gradient Tracking**\n",
        "\n",
        "**Loss Function (Cross-Entropy):**\n",
        "For each batch, compute the cross-entropy loss between the predicted token probabilities and the true next token. This measures how well the model is doing at next-token prediction.\n",
        "\n",
        "**Perplexity (PP):**\n",
        "After computing the loss, calculate perplexity as:\n",
        "**PP=exp(loss)**\n",
        "\n",
        "* Perplexity gives an interpretable measure of how many choices the model is “confused” between on average. A lower perplexity indicates better predictive performance.\n",
        "\n",
        "**Gradient Tracking for Vanishing Gradients:**\n",
        "* To observe the vanishing gradient problem, track how the gradient magnitudes change during training:\n",
        "\n",
        "* After each backpropagation step, extract the gradients of the recurrent weight matrix (e.g., `rnn.weight_hh_l0` in PyTorch).\n",
        "\n",
        "* Compute the L2 norm of these gradients.\n",
        "\n",
        "* Log or plot this value across training iterations.\n",
        "\n",
        "* If the gradients shrink toward zero, this indicates vanishing gradients; if they explode, you’ll see very large values.\n",
        "\n",
        "This task is designed to help you connect theory (cross-entropy, perplexity, vanishing gradients) to practice by giving you a direct way to measure model learning and stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a9a80e",
      "metadata": {
        "id": "78a9a80e"
      },
      "outputs": [],
      "source": [
        "def lm_loss(\n",
        "    logits: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    ignore_index: int = 0,\n",
        "    reduction: str = \"mean\"\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Compute cross-entropy loss for next-token language modeling.\n",
        "\n",
        "    Args:\n",
        "        logits: FloatTensor of shape (batch, seq_len, vocab_size).\n",
        "            Raw model outputs (unnormalized log-probabilities).\n",
        "        targets: LongTensor of shape (batch, seq_len).\n",
        "            Next-token IDs aligned with logits.\n",
        "        ignore_index: Token ID to ignore in the loss (e.g., padding).\n",
        "        reduction: 'mean' (default) or 'sum'.\n",
        "            - 'mean' → average loss across tokens\n",
        "            - 'sum' → total loss (useful for token-weighted eval)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Scalar tensor with the loss value.\n",
        "    \"\"\"\n",
        "    batch, seqlen, vocab = logits.shape\n",
        "    return F.cross_entropy(\n",
        "        logits.reshape(batch * seqlen, vocab),   # Flatten to (N, V)\n",
        "        targets.reshape(batch * seqlen),         # Flatten to (N,)\n",
        "        ignore_index=ignore_index,\n",
        "        reduction=reduction\n",
        "    )\n",
        "\n",
        "# @torch.no_grad() is a PyTorch decorator that tells PyTorch not to track gradients inside the function it decorates.\n",
        "# Any operations inside evaluate() (forward passes, loss computation, etc.) will not build a computation graph.\n",
        "# This saves memory and makes evaluation faster, because gradients aren’t needed for inference.\n",
        "# It also prevents accidental gradient updates when you’re just evaluating.\n",
        "# In short: @torch.no_grad() = “Run this function in inference mode, no gradients, no backprop.”\n",
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    pad_id: int = 0\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate model on a dataset and compute perplexity.\n",
        "\n",
        "    Uses token-weighted averaging to fairly handle batches of varying length.\n",
        "\n",
        "    Args:\n",
        "        model: Trained language model (nn.Module).\n",
        "        data_loader: DataLoader yielding (x, y) batches.\n",
        "        pad_id: Padding token ID to ignore.\n",
        "\n",
        "    Returns:\n",
        "        (mean_loss, perplexity)\n",
        "        mean_loss: Average cross-entropy per token.\n",
        "        perplexity: exp(mean_loss); interpretable measure of uncertainty.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss_sum = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for x, y in data_loader:\n",
        "        x = x.to(next(model.parameters()).device)\n",
        "        y = y.to(next(model.parameters()).device)\n",
        "\n",
        "        logits, _ = model(x)\n",
        "\n",
        "        # Sum of per-token losses\n",
        "        loss_sum = lm_loss(logits, y, ignore_index=pad_id, reduction=\"sum\")\n",
        "\n",
        "        # Count valid (non-pad) tokens\n",
        "        valid = (y != pad_id).sum().item()\n",
        "\n",
        "        total_loss_sum += loss_sum.item()\n",
        "        total_tokens += max(1, valid)\n",
        "\n",
        "    mean_loss = total_loss_sum / max(1, total_tokens)\n",
        "    perplexity = math.exp(mean_loss)\n",
        "    return mean_loss, perplexity\n",
        "\n",
        "\n",
        "def l2_grad_norm_recurrent(model: nn.Module) -> float:\n",
        "    \"\"\"Track vanishing/exploding gradients by measuring recurrent weight norms.\n",
        "\n",
        "    In RNN/LSTM/GRU modules, recurrent weights typically include 'weight_hh'\n",
        "    in their parameter names (e.g., 'rnn.weight_hh_l0').\n",
        "\n",
        "    Args:\n",
        "        model: Language model (nn.Module) after backpropagation.\n",
        "\n",
        "    Returns:\n",
        "        float: L2 norm of recurrent gradients.\n",
        "            - Near 0 → vanishing gradients.\n",
        "            - Very large → exploding gradients.\n",
        "    \"\"\"\n",
        "    total_sq = 0.0\n",
        "    count = 0\n",
        "    for name, p in model.named_parameters():\n",
        "        if (\"weight_hh\" in name) and (p.grad is not None):\n",
        "            # Compute squared L2 norm\n",
        "            total_sq += (p.grad.detach().float().norm(p=2).item()) ** 2\n",
        "            count += 1\n",
        "    return (total_sq ** 0.5) if count > 0 else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f19b36",
      "metadata": {
        "id": "25f19b36"
      },
      "source": [
        "\n",
        "## **TASK 4: Observe Vanishing Gradients: Training Vanilla RNN**\n",
        "\n",
        "We train briefly and record:\n",
        "- Training loss and PP\n",
        "- **Gradient L2 norm** of the recurrent weight (`rnn_cell.weight_hh`) after each step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4612076",
      "metadata": {
        "id": "d4612076"
      },
      "outputs": [],
      "source": [
        "def train_rnn(model: VanillaRNNLM, train_loader: DataLoader, valid_loader: DataLoader, epochs: int = 2, lr: float = 1e-3):\n",
        "    \"\"\"Train a VanillaRNNLM and log gradient norms for the recurrent matrix.\n",
        "\n",
        "    Args:\n",
        "        model: VanillaRNNLM instance.\n",
        "        train_loader: Training data loader.\n",
        "        valid_loader: Validation data loader.\n",
        "        epochs: Number of training epochs.\n",
        "        lr: Learning rate.\n",
        "\n",
        "    Returns:\n",
        "        A dict with training logs: losses, pps, grad_norms, val_history.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    logs = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_pp\": [],\n",
        "        \"grad_norm_rhh\": [],\n",
        "        \"val\": [],\n",
        "    }\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"RNN Epoch {ep}\")\n",
        "        for x, y in pbar:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits, _ = model(x)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = lm_loss(logits, y)\n",
        "\n",
        "            # Backward\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "\n",
        "            # Record gradient norm of recurrent weight (a common place to see vanishing)\n",
        "            rhh = model.rnn_cell.weight_hh\n",
        "            grad_norm = rhh.grad.detach().norm(2).item() if rhh.grad is not None else 0.0\n",
        "\n",
        "            # Clip to avoid exploding for fairness (small value)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update\n",
        "            opt.step()\n",
        "\n",
        "            logs[\"train_loss\"].append(loss.item())\n",
        "            logs[\"train_pp\"].append(math.exp(loss.item()))\n",
        "            logs[\"grad_norm_rhh\"].append(grad_norm)\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.3f}\", pp=f\"{math.exp(loss.item()):.1f}\", grad=f\"{grad_norm:.4e}\")\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_pp = evaluate(model, valid_loader)\n",
        "        logs[\"val\"].append((val_loss, val_pp))\n",
        "        print(f\"[RNN] Epoch {ep}: val_loss={val_loss:.3f}, val_pp={val_pp:.1f}\")\n",
        "\n",
        "    return logs, model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef634c29",
      "metadata": {
        "id": "ef634c29"
      },
      "source": [
        "\n",
        "## **TASK 5: Train LSTM And Compare the Results to Vanilla RNN below**\n",
        "\n",
        "We repeat with an **LSTM** and track the gradient norm of its recurrent weight (`weight_hh_l0`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d34e497",
      "metadata": {
        "id": "2d34e497"
      },
      "outputs": [],
      "source": [
        "def _find_lstm_recurrent_weight(model: nn.Module) -> Optional[torch.nn.Parameter]:\n",
        "    \"\"\"Best-effort lookup for the first layer's recurrent weight matrix in an LSTM.\n",
        "\n",
        "    LSTM parameter names in PyTorch typically look like:\n",
        "        - weight_ih_l0  (input-to-hidden, layer 0)\n",
        "        - weight_hh_l0  (hidden-to-hidden, layer 0)  ← recurrent matrix we want\n",
        "      If the model is wrapped (e.g., inside a module attr like `model.lstm`\n",
        "      or DataParallel), we search by substring.\n",
        "\n",
        "    Args:\n",
        "        model: LSTM language model (must contain an nn.LSTM module).\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Parameter or None if not found.\n",
        "    \"\"\"\n",
        "    # Common direct attribute path: model.lstm.weight_hh_l0\n",
        "    if hasattr(model, \"lstm\") and hasattr(model.lstm, \"weight_hh_l0\"):\n",
        "        return model.lstm.weight_hh_l0\n",
        "\n",
        "    # Fallback: scan parameters by name for the first layer recurrent weight\n",
        "    for name, p in model.named_parameters():\n",
        "        # Match both layer 0 and a generic 'weight_hh' just in case\n",
        "        if (\"weight_hh_l0\" in name) or (name.endswith(\"weight_hh\") and p.ndimension() == 2):\n",
        "            return p\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def train_lstm(\n",
        "    model: \"LSTMLM\",\n",
        "    train_loader: DataLoader,\n",
        "    valid_loader: DataLoader,\n",
        "    epochs: int = 2,\n",
        "    lr: float = 1e-3\n",
        ") -> Tuple[Dict[str, List[float]], \"LSTMLM\"]:\n",
        "    \"\"\"Train an LSTM language model; log perplexity & recurrent gradient norms.\n",
        "\n",
        "    This mirrors the vanilla RNN trainer so you can compare *vanishing/exploding*\n",
        "    behavior across architectures under similar training conditions.\n",
        "\n",
        "    Training loop (per batch):\n",
        "      1) Forward pass → logits over vocab for each time step\n",
        "      2) Cross-entropy loss against next-token targets\n",
        "      3) Backpropagate gradients\n",
        "      4) Record L2 norm of *recurrent* matrix grads (weight_hh_l0)\n",
        "      5) (Gentle) gradient clipping to tame explosions\n",
        "      6) Optimizer step\n",
        "      7) Log loss, perplexity, grad norm\n",
        "\n",
        "    Notes:\n",
        "      • We log the L2 norm of `weight_hh_l0` gradients. RNNs tend to show more\n",
        "        vanishing; LSTMs often mitigate it via gating, so you should see larger,\n",
        "        healthier gradient norms on average—but still decaying with long sequences.\n",
        "      • If your pad id ≠ 0, set it in your `lm_loss` / `evaluate` helpers.\n",
        "      • Ensure your `model(x)` returns (logits, state) where logits=(B, T, V).\n",
        "\n",
        "    Args:\n",
        "        model: LSTMLM instance (embedding → LSTM → linear-to-vocab).\n",
        "        train_loader: Dataloader yielding (x, y) training batches.\n",
        "        valid_loader: Dataloader yielding (x, y) validation batches.\n",
        "        epochs: Number of full passes over the training set.\n",
        "        lr: Learning rate for Adam.\n",
        "\n",
        "    Returns:\n",
        "        (logs, model)\n",
        "        logs:\n",
        "          - \"train_loss\":    per-step cross-entropy (float)\n",
        "          - \"train_pp\":      per-step perplexity = exp(loss)\n",
        "          - \"grad_norm_hh\":  per-step L2 norm of recurrent weight grads\n",
        "          - \"val\":           per-epoch (val_loss, val_pp)\n",
        "        model: The trained model (same object, updated in-place).\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    logs: Dict[str, List] = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_pp\": [],\n",
        "        \"grad_norm_hh\": [],\n",
        "        \"val\": [],\n",
        "    }\n",
        "\n",
        "    # Resolve the recurrent matrix once (will still check grad presence each step)\n",
        "    recurrent_w = _find_lstm_recurrent_weight(model)\n",
        "    if recurrent_w is None:\n",
        "        print(\"[warn] Could not find LSTM recurrent weight; grad tracking will be 0.0. \"\n",
        "              \"Adapt `_find_lstm_recurrent_weight` for your model structure.\")\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"LSTM Epoch {ep}\")\n",
        "\n",
        "        for x, y in pbar:\n",
        "            # ---- 1) Move batch to device\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # ---- 2) Forward pass: logits shape (B, T, V); state unused here\n",
        "            logits, _ = model(x)\n",
        "\n",
        "            # ---- 3) Cross-entropy over next-token targets (ignore padding id=0 by default)\n",
        "            loss = lm_loss(logits, y, ignore_index=0, reduction=\"mean\")\n",
        "\n",
        "            # ---- 4) Backpropagation\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "\n",
        "            # ---- 5) Recurrent gradient L2 norm (vanishing/exploding signal)\n",
        "            # Tracking this norm shows whether gradients are vanishing (very close to zero)\n",
        "            # or exploding (very large). It’s a diagnostic tool for training stability in RNNs and LSTMs.\n",
        "\n",
        "            # The if statement below ensures the recurrent weight matrix exists (e.g., the hidden-to-hidden weight in an RNN cell).\n",
        "            # and makes sure a gradient has been computed for it (i.e., after loss.backward()).\n",
        "            if recurrent_w is not None and recurrent_w.grad is not None:\n",
        "                #Breaks the gradient tensor from the computation graph, so we don’t accidentally backpropagate again and\n",
        "                # Converts the tensor to float type to avoid precision issues. norm(p=2) computes the L2 norm (Euclidean length)\n",
        "                # of the gradient vector. This tells us the overall “size” of the gradient update for this weight matrix.\n",
        "                grad_norm = recurrent_w.grad.detach().float().norm(p=2).item()\n",
        "            else:\n",
        "                #If the weight or its gradient doesn’t exist, it safely records the gradient norm as zero.\n",
        "                grad_norm = 0.0\n",
        "\n",
        "            # ---- 6) Gentle global clipping helps with explosions without hiding vanishing\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # ---- 7) Parameter update\n",
        "            opt.step()\n",
        "\n",
        "            # ---- 8) Metrics & live logging\n",
        "            loss_val = loss.item()\n",
        "            pp_val = math.exp(loss_val)\n",
        "            logs[\"train_loss\"].append(loss_val)\n",
        "            logs[\"train_pp\"].append(pp_val)\n",
        "            logs[\"grad_norm_hh\"].append(grad_norm)\n",
        "\n",
        "            pbar.set_postfix(\n",
        "                loss=f\"{loss_val:.3f}\",\n",
        "                pp=f\"{pp_val:.1f}\",\n",
        "                grad=f\"{grad_norm:.4e}\"\n",
        "            )\n",
        "\n",
        "        # ---- 9) End-of-epoch validation (token-weighted mean loss + perplexity)\n",
        "        val_loss, val_pp = evaluate(model, valid_loader)  # assumes ignore_index inside\n",
        "        logs[\"val\"].append((val_loss, val_pp))\n",
        "        print(f\"[LSTM] Epoch {ep}: val_loss={val_loss:.3f}, val_pp={val_pp:.1f}\")\n",
        "\n",
        "    return logs, model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70fdfb04",
      "metadata": {
        "id": "70fdfb04"
      },
      "source": [
        "\n",
        "## **TASK 6) Run Experiments and Discuss the Results**\n",
        "\n",
        "We keep the setup small so it trains quickly, but you should still observe the key differences between architectures:\n",
        "\n",
        "**Vanilla RNN**\n",
        "\n",
        "Recurrent gradient norms often shrink toward zero, illustrating the vanishing gradient problem.\n",
        "\n",
        "Struggles to capture longer-range dependencies, leading to higher perplexity (worse language modeling performance).\n",
        "\n",
        "**LSTM**\n",
        "\n",
        "Thanks to gating mechanisms (input/forget/output), gradient flow is healthier, norms stay larger and more stable.\n",
        "\n",
        "Handles longer context better, resulting in lower perplexity (better predictive performance).\n",
        "\n",
        "As you compare plots of perplexity and gradient norms, think about:\n",
        "\n",
        "* How the theory of vanishing gradients is visible in practice.\n",
        "\n",
        "* Why LSTMs, despite being more complex, are still the default for many sequence tasks before Transformers.\n",
        "\n",
        "* What tradeoffs you might expect if you scaled up sequence length, hidden size, or training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c9096bb",
      "metadata": {
        "id": "4c9096bb"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 1337) -> None:\n",
        "    \"\"\"Fix random seeds for fair comparisons.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1337)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Config (shared across models)\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class ExpConfig:\n",
        "    emb: int = 128\n",
        "    hid: int = 256\n",
        "    epochs: int = 2\n",
        "    lr: float = 1e-3\n",
        "    pad_idx: int = 0\n",
        "\n",
        "CFG = ExpConfig()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vanilla RNN\n",
        "rnn_model = VanillaRNNLM(\n",
        "    vocab_size=vocab_size,\n",
        "    emb_dim=CFG.emb,\n",
        "    hidden_dim=CFG.hid,\n",
        "    pad_idx=CFG.pad_idx if hasattr(VanillaRNNLM, \"__init__\") else 0  # optional\n",
        ").to(device)\n",
        "rnn_logs, rnn_model = train_rnn(\n",
        "    rnn_model, train_loader, valid_loader, epochs=CFG.epochs, lr=CFG.lr\n",
        ")\n",
        "\n",
        "# LSTM\n",
        "lstm_model = LSTMLM(\n",
        "    vocab_size=vocab_size,\n",
        "    emb_dim=CFG.emb,\n",
        "    hidden_dim=CFG.hid,\n",
        "    pad_idx=CFG.pad_idx if hasattr(LSTMLM, \"__init__\") else 0  # optional\n",
        ").to(device)\n",
        "lstm_logs, lstm_model = train_lstm(\n",
        "    lstm_model, train_loader, valid_loader, epochs=CFG.epochs, lr=5e-4\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Summarize results\n",
        "# ----------------------------\n",
        "def summarize_run(name: str, logs: Dict) -> Dict[str, float]:\n",
        "    \"\"\"Create a compact summary for printing/plotting.\"\"\"\n",
        "    last_train_loss = logs[\"train_loss\"][-1] if logs[\"train_loss\"] else float(\"nan\")\n",
        "    last_train_pp   = logs[\"train_pp\"][-1]   if logs[\"train_pp\"]   else float(\"nan\")\n",
        "    last_grad_norm  = logs[\"grad_norm_rhh\"][-1] if \"grad_norm_rhh\" in logs and logs[\"grad_norm_rhh\"] \\\n",
        "                      else (logs[\"grad_norm_hh\"][-1] if \"grad_norm_hh\" in logs and logs[\"grad_norm_hh\"] else float(\"nan\"))\n",
        "    last_val_loss, last_val_pp = logs[\"val\"][-1] if logs[\"val\"] else (float(\"nan\"), float(\"nan\"))\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"train_loss\": last_train_loss,\n",
        "        \"train_pp\": last_train_pp,\n",
        "        \"val_loss\": last_val_loss,\n",
        "        \"val_pp\": last_val_pp,\n",
        "        \"last_grad_L2_recurrent\": last_grad_norm,\n",
        "    }\n",
        "\n",
        "rnn_summary  = summarize_run(\"VanillaRNN\", rnn_logs)\n",
        "lstm_summary = summarize_run(\"LSTM\",        lstm_logs)\n",
        "\n",
        "# Pretty print a quick table\n",
        "def _fmt(x: float) -> str:\n",
        "    return f\"{x:.3f}\" if isinstance(x, float) and x == x else \"n/a\"\n",
        "\n",
        "print(\"\\n=== Summary (same config, different cells) ===\")\n",
        "print(f\"{'Model':12}  {'TrainLoss':>10}  {'TrainPP':>10}  {'ValLoss':>10}  {'ValPP':>10}  {'GradL2(hh)':>12}\")\n",
        "for row in (rnn_summary, lstm_summary):\n",
        "    print(f\"{row['model']:12}  \"\n",
        "          f\"{_fmt(row['train_loss']):>10}  {_fmt(row['train_pp']):>10}  \"\n",
        "          f\"{_fmt(row['val_loss']):>10}  {_fmt(row['val_pp']):>10}  {_fmt(row['last_grad_L2_recurrent']):>12}\")\n"
      ],
      "metadata": {
        "id": "F6J4VPaLM9wN"
      },
      "id": "F6J4VPaLM9wN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6268b0cd",
      "metadata": {
        "id": "6268b0cd"
      },
      "source": [
        "\n",
        "## **Observation: Visualize Perplexity and Gradient Norms**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cfae9e6",
      "metadata": {
        "id": "9cfae9e6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Training Loss (all steps) – complements our Train PP slice\n",
        "plt.figure()\n",
        "plt.plot(rnn_logs[\"train_loss\"], label=\"RNN loss\")\n",
        "plt.plot(lstm_logs[\"train_loss\"], label=\"LSTM loss\")\n",
        "plt.xlabel(\"Update step\")\n",
        "plt.ylabel(\"Cross-Entropy (train)\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Loss (all steps)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2) (Optional) Smoothed Train PP with a rolling mean\n",
        "def moving_avg(xs, k=20):\n",
        "    import numpy as np\n",
        "    if len(xs) < k: return xs\n",
        "    c = np.cumsum([0.0] + xs)\n",
        "    return [(c[i+k]-c[i])/k for i in range(len(xs)-k+1)]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(moving_avg(rnn_logs[\"train_pp\"][:300]), label=\"RNN PP (smoothed)\")\n",
        "plt.plot(moving_avg(lstm_logs[\"train_pp\"][:300]), label=\"LSTM PP (smoothed)\")\n",
        "plt.xlabel(\"Update step (smoothed)\")\n",
        "plt.ylabel(\"Perplexity (train)\")\n",
        "plt.yscale(\"log\")  # helps when values span orders of magnitude\n",
        "plt.legend(); plt.title(\"Train Perplexity (first 300 steps, smoothed)\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# 3) Gradient norms, plus log-scale for visibility if needed\n",
        "plt.figure()\n",
        "plt.plot(rnn_logs[\"grad_norm_rhh\"][:300], label=\"RNN ‖∇(hh)‖₂\")\n",
        "plt.plot(lstm_logs[\"grad_norm_hh\"][:300], label=\"LSTM ‖∇(hh_l0)‖₂\")\n",
        "plt.xlabel(\"Update step\")\n",
        "plt.ylabel(\"L2 grad norm\")\n",
        "plt.yscale(\"log\")  # helps when values span orders of magnitude\n",
        "plt.legend(); plt.title(\"Recurrent Gradient Norms (first 300 steps, log scale)\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# 4) Validation PP per epoch\n",
        "rnn_val_pp  = [pp for _, pp in rnn_logs[\"val\"]]\n",
        "lstm_val_pp = [pp for _, pp in lstm_logs[\"val\"]]\n",
        "plt.figure()\n",
        "plt.plot(rnn_val_pp, marker=\"o\", label=\"RNN val PP\")\n",
        "plt.plot(lstm_val_pp, marker=\"o\", label=\"LSTM val PP\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Validation PP\")\n",
        "plt.legend(); plt.title(\"Validation Perplexity by Epoch\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "545523d5",
      "metadata": {
        "id": "545523d5"
      },
      "source": [
        "\n",
        "## **8) Discussion: Explain in 2–3 sentences**\n",
        "\n",
        "- What evidence in the plots suggests **vanishing gradients** for the vanilla RNN?\n",
        "  -   - The RNN’s gradient norm curve drops sharply over training steps, approaching values near zero on a log scale. This exponential decay shows the gradient signal dying out, which means earlier time steps cannot influence weight updates, classic vanishing gradient behavior.\n",
        "- How do **perplexity** trends differ between RNN and LSTM?\n",
        "  - For vanilla RNNs, perplexity decreases at first but then plateaus at a higher value, showing the model cannot capture long dependencies. LSTMs continue reducing perplexity more steadily and reach lower values, because the cell state and gates preserve long-term information.\n",
        "- If you increased `SEQ_LEN` (unroll length), how would you expect the RNN's gradient norms to change? Why?\n",
        "  - The gradient norms for the vanilla RNN would shrink even faster. Longer unrolls = more Jacobian multiplications in BPTT, so if each multiplier is < 1 in norm, the product decays exponentially quicker. Result: gradients vanish more severely, making training unstable for long contexts.\n",
        "- If you extend SEQ_LEN from 30 to 90 with the same LR, what happens to RNN grads and why do LSTM gates help?\n",
        "  - For RNNs: Gradients vanish almost completely, since the signal must travel through 3× as many steps; weight updates for long-range dependencies approach zero.\n",
        "  - For LSTMs: The gating mechanism allows information to flow along the cell state with minimal decay. Forget/input/output gates decide what to keep, update, or reveal, so the effective gradient doesn’t vanish as badly. This enables LSTMs to remain trainable even with longer unroll lengths.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4cpmmclHQGiw"
      },
      "id": "4cpmmclHQGiw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}