{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2205a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use GPU when available for faster training/visuals; fall back to CPU otherwise\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using', DEVICE)\n",
    "\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-v1\", verification_mode=\"no_checks\")\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# enc = tokenizer(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036023d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled dot-product attention shapes OK: torch.Size([2, 3, 4, 8]) torch.Size([2, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor|None=None):\n",
    "    \"\"\"Compute scaled dot-product attention.\n",
    "\n",
    "    Implements the equation from \"Attention is All You Need\":\n",
    "\n",
    "        Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k) + mask) V\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor of shape (B, h, T_q, d_k).\n",
    "        K: Key tensor of shape (B, h, T_k, d_k).\n",
    "        V: Value tensor of shape (B, h, T_k, d_v).\n",
    "        mask: Optional additive mask broadcastable to (B, h, T_q, T_k) with 0 or -inf.\n",
    "\n",
    "    Returns:\n",
    "        out: Attention output, shape (B, h, T_q, d_v).\n",
    "        attn: Attention weights (softmax probabilities), shape (B, h, T_q, T_k).\n",
    "    \"\"\"\n",
    "    # d_k is the dimensionality of queries/keys per head\n",
    "    d_k = Q.size(-1)  # read last dimension of Q for scaling\n",
    "\n",
    "    # Compute raw attention scores by matrix-multiplying Q and K^T\n",
    "    # Q @ K^T yields shape (B, h, T_q, T_k)\n",
    "    #TODO\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)# scale by sqrt(d_k)\n",
    "\n",
    "    # If a mask was provided, add it to the scores. Mask entries are 0 (keep) or -inf (block)\n",
    "    if mask is not None:\n",
    "        # Ensure mask is same dtype and on same device as scores to avoid runtime errors\n",
    "        mask = mask.to(dtype=scores.dtype, device=scores.device)\n",
    "        scores = scores + mask  # additive masking prior to softmax\n",
    "\n",
    "    # Convert scores to probabilities along the key dimension with softmax\n",
    "    # use torch functional library you important above, which is a PyTorch\n",
    "    # module containing functional (stateless) implementations of layers\n",
    "    # and operations like softmax, relu, cross_entropy, etc.\n",
    "    #TODO\n",
    "    attn = F.softmax(scores, dim=-1)  # softmax over T_k\n",
    "\n",
    "    # Use attention weights to produce weighted sum over values\n",
    "    # This line of code will perform a batched matrix multiplication over the last two dimensions\n",
    "    out = torch.matmul(attn, V) # (B, h, T_q, d_v)\n",
    "\n",
    "    # Return both the attended outputs and the attention weights for inspection\n",
    "    return out, attn\n",
    "\n",
    "# Quick shape test: verify the function returns expected tensor shapes\n",
    "B, h, T, d_k, d_v = 2, 3, 4, 8, 8  # batch, heads, time, key-dim, value-dim\n",
    "Q = torch.randn(B, h, T, d_k)  # random queries\n",
    "K = torch.randn(B, h, T, d_k)  # random keys\n",
    "V = torch.randn(B, h, T, d_v)  # random values\n",
    "out, attn = scaled_dot_product_attention(Q, K, V)  # call the function\n",
    "assert out.shape == (B, h, T, d_v) and attn.shape == (B, h, T, T)  # sanity assert\n",
    "print('Scaled dot-product attention shapes OK:', out.shape, attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd7dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_positional_encoding(T:int, d_model:int, device=DEVICE):\n",
    "    \"\"\"Create sinusoidal positional encodings.\n",
    "\n",
    "    Implements the original formulation from Vaswani et al. where each dimension\n",
    "    of the positional encoding uses a different frequency.\n",
    "\n",
    "    Args:\n",
    "        T: Sequence length (number of positions).\n",
    "        d_model: Model dimensionality (must be even to pair sin/cos dims nicely).\n",
    "        device: Torch device for the returned tensor.\n",
    "\n",
    "    Returns:\n",
    "        PE: Tensor of shape (T, d_model) containing positional encodings.\n",
    "    \"\"\"\n",
    "    # Ensure d_model is even so even/odd pairing works\n",
    "    assert d_model % 2 == 0, \"d_model must be even for sinusoidal positional encoding\"\n",
    "\n",
    "    # position indices (T, 1) as float\n",
    "    pos = torch.arange(T, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # dimension indices (1, d_model) as float\n",
    "    i = torch.arange(d_model, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # compute the rate term 1/10000^{2i/d_model}\n",
    "    angle_rates = 1.0 / torch.pow(10000.0, (2 * (i // 2)) / d_model)\n",
    "\n",
    "    # outer product to get angles for every position and dimension\n",
    "    angles = pos * angle_rates  # (T, d_model)\n",
    "\n",
    "    # allocate and fill even/odd indices with sin/cos\n",
    "    PE = torch.zeros((T, d_model), device=device)\n",
    "    PE[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "    PE[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "    return PE\n",
    "\n",
    "\n",
    "def causal_mask(T_q: int, T_k: int, device=DEVICE, dtype: torch.dtype=torch.float32):\n",
    "    \"\"\"Create an additive causal mask to prevent attention to future positions.\n",
    "\n",
    "    The mask returned can be added directly to attention logits before softmax.\n",
    "\n",
    "    Args:\n",
    "        T_q: Number of query positions.\n",
    "        T_k: Number of key/value positions.\n",
    "        device: Torch device to create the mask on.\n",
    "        dtype: Desired floating dtype for the returned mask (default: torch.float32).\n",
    "\n",
    "    Returns:\n",
    "        mask: Tensor of shape (1, 1, T_q, T_k) with 0.0 where allowed and -inf where masked.\n",
    "    \"\"\"\n",
    "    # Allocate a mask filled with -inf (all positions masked initially) with requested dtype\n",
    "    mask = torch.full((1,1,T_q,T_k), float('-inf'), device=device, dtype=dtype)\n",
    "\n",
    "    # Build a lower-triangular matrix of ones (allowed positions are 1)\n",
    "    tril = torch.tril(torch.ones(T_q, T_k, device=device, dtype=dtype))\n",
    "\n",
    "    # Wherever tril == 1, set the mask value to 0.0 (meaning \"allowed\")\n",
    "    mask = mask.masked_fill(tril == 1, 0.0)\n",
    "\n",
    "    # Return mask shaped (1,1,T_q,T_k) which will broadcast over batch and heads\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374ac815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny MHA out shape: torch.Size([2, 5, 32]) | attn: torch.Size([2, 4, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class TinyMultiHeadAttention(nn.Module):\n",
    "    \"\"\"A minimal multi-head self-attention implementation.\n",
    "\n",
    "    This class implements the core mechanics of multi-head attention without\n",
    "    dropout or biases. It projects inputs to Q/K/V, splits into heads, applies\n",
    "    scaled dot-product attention per head, and concatenates the results.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        # Ensure d_model is divisible by number of heads for equal head size\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model  # full model dimensionality\n",
    "        self.num_heads = num_heads  # number of parallel attention heads\n",
    "        self.d_k = d_model // num_heads  # dimensionality per head\n",
    "\n",
    "        # Linear projections for queries, keys and values (project then split into heads)\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)  # projects input -> Q_all\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)  # projects input -> K_all\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)  # projects input -> V_all\n",
    "\n",
    "        # Output linear projection that combines concatenated head outputs\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)  # projects heads concat -> output\n",
    "\n",
    "    def split_heads(self, X):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k) and transpose.\n",
    "\n",
    "        Args:\n",
    "            X: Tensor of shape (B, T, D)\n",
    "        Returns:\n",
    "            Tensor of shape (B, h, T, d_k)\n",
    "        \"\"\"\n",
    "        # Unpack batch, time, and feature dims\n",
    "        B, T, D = X.shape\n",
    "        # Reshape to separate heads and then transpose head dim upfront\n",
    "        X = X.view(B, T, self.num_heads, self.d_k).transpose(1,2)  # (B,h,T,d_k)\n",
    "        return X\n",
    "\n",
    "    def combine_heads(self, X):\n",
    "        \"\"\"Inverse of split_heads: transpose and merge heads into feature dim.\n",
    "\n",
    "        Args:\n",
    "            X: Tensor of shape (B, h, T, d_k)\n",
    "        Returns:\n",
    "            Tensor of shape (B, T, D)\n",
    "        \"\"\"\n",
    "        # Unpack shapes\n",
    "        B, h, T, d_k = X.shape\n",
    "        # Transpose to (B, T, h, d_k) then flatten the last two dims\n",
    "        X = X.transpose(1,2).contiguous().view(B, T, h*d_k)  # (B,T,D)\n",
    "        return X\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"Forward pass for TinyMultiHeadAttention.\n",
    "\n",
    "        Args:\n",
    "            X: Input tensor of shape (B, T, D=d_model).\n",
    "            mask: Optional additive mask to apply to attention logits.\n",
    "\n",
    "        Returns:\n",
    "            out_proj: Output tensor of shape (B, T, D).\n",
    "            attn: Attention weights from scaled_dot_product_attention (B, h, T, T).\n",
    "        \"\"\"\n",
    "        # Project inputs to combined Q/K/V of shape (B, T, D)\n",
    "        Q_all = self.W_q(X)  # (B, T, D)\n",
    "        K_all = self.W_k(X)  # (B, T, D)\n",
    "        V_all = self.W_v(X)  # (B, T, D)\n",
    "\n",
    "        # Split the combined Q/K/V into multiple heads: (B, h, T, d_k)\n",
    "        Q = self.split_heads(Q_all)\n",
    "        K = self.split_heads(K_all)\n",
    "        V = self.split_heads(V_all)\n",
    "\n",
    "        # Compute attention per head using scaled dot-product attention\n",
    "        out, attn = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine head outputs back into (B, T, D)\n",
    "        out_combined = self.combine_heads(out)\n",
    "\n",
    "        # Final linear projection\n",
    "        out_proj = self.W_o(out_combined)\n",
    "\n",
    "        return out_proj, attn\n",
    "\n",
    "# Sanity check\n",
    "B,T,D,h = 2,5,32,4\n",
    "x = torch.randn(B,T,D)\n",
    "mha = TinyMultiHeadAttention(D,h)\n",
    "y, attn = mha(x)\n",
    "print('Tiny MHA out shape:', y.shape, '| attn:', attn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850ade7",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a75336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Hyperparameters for data + batching\n",
    "# block_size = 128            # tokens per training example\n",
    "# batch_size = 16\n",
    "# max_train_sequences = 4000  # total training chunks (4000 * 128 = 512k tokens)\n",
    "# max_val_sequences = 512     # total validation chunks\n",
    "# max_train_docs = 60000      # number of WikiText-2 lines to concatenate for training\n",
    "# max_val_docs = 6000\n",
    "\n",
    "# # Reuse the WikiText-2 dataset already loaded above (variable `ds`)\n",
    "# def build_corpus(split, max_docs):\n",
    "#     subset = split.select(range(min(len(split), max_docs)))\n",
    "#     return \"\\n\\n\".join(subset[\"text\"])\n",
    "\n",
    "# train_text = build_corpus(ds[\"train\"], max_docs=max_train_docs)\n",
    "# val_text = build_corpus(ds[\"validation\"], max_docs=max_val_docs)\n",
    "\n",
    "# # GPT-2 tokenizer is byte-level BPE; reuse it for consistency with modern decoders\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", model_max_length=block_size)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# max_train_tokens = block_size * max_train_sequences\n",
    "# max_val_tokens = block_size * max_val_sequences\n",
    "\n",
    "# def ids_to_chunks(token_ids, max_tokens):\n",
    "#     usable = min(len(token_ids), max_tokens)\n",
    "#     usable = (usable // block_size) * block_size\n",
    "#     tensor = torch.tensor(token_ids[:usable], dtype=torch.long)\n",
    "#     return tensor.view(-1, block_size)\n",
    "\n",
    "# train_ids = tokenizer(train_text, add_special_tokens=False, return_attention_mask=False)[\"input_ids\"]\n",
    "# val_ids = tokenizer(val_text, add_special_tokens=False, return_attention_mask=False)[\"input_ids\"]\n",
    "\n",
    "# train_chunks = ids_to_chunks(train_ids, max_train_tokens)\n",
    "# val_chunks = ids_to_chunks(val_ids, max_val_tokens)\n",
    "\n",
    "# print(f\"Prepared {train_chunks.shape[0]} train chunks ({train_chunks.numel():,} tokens)\")\n",
    "# print(f\"Prepared {val_chunks.shape[0]} val chunks   ({val_chunks.numel():,} tokens)\")\n",
    "\n",
    "# train_dataset = TensorDataset(train_chunks)\n",
    "# val_dataset = TensorDataset(val_chunks)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748c523",
   "metadata": {},
   "source": [
    "using BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abdd417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 17611 train chunks (2,254,208 tokens)\n",
      "Prepared 1024 val chunks   (131,072 tokens)\n"
     ]
    }
   ],
   "source": [
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "# block_size = 128\n",
    "# batch_size = 16\n",
    "# max_train_sequences = 20000\n",
    "# max_val_sequences = 1024\n",
    "# max_train_docs = 200000\n",
    "# max_val_docs = 10000\n",
    "# dropout = 0.1\n",
    "\n",
    "# def build_corpus(split, max_docs):\n",
    "#     subset = split.select(range(min(len(split), max_docs)))\n",
    "#     return \"\\n\\n\".join(subset[\"text\"])\n",
    "\n",
    "# train_text = build_corpus(ds[\"train\"], max_docs=max_train_docs)\n",
    "# val_text = build_corpus(ds[\"validation\"], max_docs=max_val_docs)\n",
    "\n",
    "# # Save text temporarily so ByteLevelBPETokenizer can read it\n",
    "# os.makedirs(\"tokenizer_data\", exist_ok=True)\n",
    "# with open(\"tokenizer_data/train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(train_text)\n",
    "# with open(\"tokenizer_data/val.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(val_text)\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# tokenizer.train(\n",
    "#     files=[\"tokenizer_data/train.txt\"],\n",
    "#     vocab_size=50257,\n",
    "#     min_frequency=2,\n",
    "#     special_tokens=[\n",
    "#         \"<unk>\",\n",
    "#         \"<pad>\",\n",
    "#         \"<bos>\",\n",
    "#         \"<eos>\",\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# def encode_text(text):\n",
    "#     return tokenizer.encode(text).ids\n",
    "\n",
    "# train_ids = encode_text(train_text)\n",
    "# val_ids = encode_text(val_text)\n",
    "\n",
    "# max_train_tokens = block_size * max_train_sequences\n",
    "# max_val_tokens = block_size * max_val_sequences\n",
    "\n",
    "# def ids_to_chunks(token_ids, max_tokens):\n",
    "#     usable = min(len(token_ids), max_tokens)\n",
    "#     usable = (usable // block_size) * block_size\n",
    "#     tensor = torch.tensor(token_ids[:usable], dtype=torch.long)\n",
    "#     return tensor.view(-1, block_size)\n",
    "\n",
    "# train_chunks = ids_to_chunks(train_ids, max_train_tokens)\n",
    "# val_chunks = ids_to_chunks(val_ids, max_val_tokens)\n",
    "\n",
    "# print(f\"Prepared {train_chunks.shape[0]} train chunks ({train_chunks.numel():,} tokens)\")\n",
    "# print(f\"Prepared {val_chunks.shape[0]} val chunks   ({val_chunks.numel():,} tokens)\")\n",
    "\n",
    "# train_dataset = TensorDataset(train_chunks)\n",
    "# val_dataset = TensorDataset(val_chunks)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b78e8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 17095 train chunks (2,188,160 tokens)\n",
      "Prepared 1024 val chunks   (131,072 tokens)\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 16\n",
    "max_train_sequences = 20000\n",
    "max_val_sequences = 1024\n",
    "max_train_docs = 200000\n",
    "max_val_docs = 10000\n",
    "dropout = 0.0\n",
    "\n",
    "def build_corpus(split, max_docs):\n",
    "    subset = split.select(range(min(len(split), max_docs)))\n",
    "    return \"\\n\\n\".join(subset[\"text\"])\n",
    "\n",
    "train_text = build_corpus(ds[\"train\"], max_docs=max_train_docs)\n",
    "val_text = build_corpus(ds[\"validation\"], max_docs=max_val_docs)\n",
    "\n",
    "# Save text for sentencepiece training\n",
    "os.makedirs(\"tokenizer_data\", exist_ok=True)\n",
    "with open(\"tokenizer_data/train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(train_text)\n",
    "with open(\"tokenizer_data/val.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(val_text)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Train SentencePiece tokenizer\n",
    "# ------------------------------------------------------------\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=\"tokenizer_data/train.txt\",\n",
    "    model_prefix=\"spm_model\",\n",
    "    vocab_size=50257,               # adjust as needed\n",
    "    character_coverage=1.0,         # 1.0 for English-only corpora\n",
    "    model_type=\"bpe\",           # options: unigram | bpe | word | char\n",
    "    unk_id=0,\n",
    "    pad_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    user_defined_symbols=[],\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm_model.model\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Encoding function using SentencePiece\n",
    "# ------------------------------------------------------------\n",
    "def encode_text(text):\n",
    "    return sp.encode(text, out_type=int)\n",
    "\n",
    "train_ids = encode_text(train_text)\n",
    "val_ids  = encode_text(val_text)\n",
    "\n",
    "max_train_tokens = block_size * max_train_sequences\n",
    "max_val_tokens   = block_size * max_val_sequences\n",
    "\n",
    "def ids_to_chunks(token_ids, max_tokens):\n",
    "    usable = min(len(token_ids), max_tokens)\n",
    "    usable = (usable // block_size) * block_size\n",
    "    tensor = torch.tensor(token_ids[:usable], dtype=torch.long)\n",
    "    return tensor.view(-1, block_size)\n",
    "\n",
    "train_chunks = ids_to_chunks(train_ids, max_train_tokens)\n",
    "val_chunks   = ids_to_chunks(val_ids, max_val_tokens)\n",
    "\n",
    "print(f\"Prepared {train_chunks.shape[0]} train chunks ({train_chunks.numel():,} tokens)\")\n",
    "print(f\"Prepared {val_chunks.shape[0]} val chunks   ({val_chunks.numel():,} tokens)\")\n",
    "\n",
    "train_dataset = TensorDataset(train_chunks)\n",
    "val_dataset   = TensorDataset(val_chunks)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1842b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Transformer decoder block with pre-norm residual layout.\"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, mlp_ratio: int = 4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = TinyMultiHeadAttention(d_model, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, mlp_ratio * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * d_model, d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_out, attn_weights = self.attn(self.ln1(x), mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        # x = x + attn_out\n",
    "        ff_out = self.ff(self.ln2(x))\n",
    "        x = x + self.dropout(ff_out)\n",
    "        # x = x + ff_out\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"Compact decoder-only transformer for language modeling.\"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int = 256, num_layers: int = 4,\n",
    "                 num_heads: int = 4, block_size: int = 128):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        pe = sinusoidal_positional_encoding(block_size, d_model)\n",
    "        self.register_buffer(\"pos_emb\", pe.unsqueeze(0))  # (1, block_size, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, return_attn: bool = False):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            idx = idx[:, -self.block_size:]\n",
    "            T = idx.shape[1]\n",
    "        tok = self.token_emb(idx)\n",
    "        pos = self.pos_emb[:, :T, :]\n",
    "        x = tok + pos\n",
    "        mask = causal_mask(T, T, device=idx.device, dtype=tok.dtype)\n",
    "        attn_maps = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x, mask)\n",
    "            if return_attn:\n",
    "                attn_maps.append(attn)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if return_attn:\n",
    "            return logits, attn_maps\n",
    "        return logits, None\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    # def generate(self, idx, max_new_tokens: int = 50, temperature: float = 1.0, top_k: int | None = 50):\n",
    "    #     self.eval()\n",
    "    #     idx = idx.clone()\n",
    "    #     for _ in range(max_new_tokens):\n",
    "    #         idx_cond = idx[:, -self.block_size:]\n",
    "    #         logits, _ = self(idx_cond)\n",
    "    #         logits = logits[:, -1, :] / temperature\n",
    "    #         if top_k is not None:\n",
    "    #             topk_vals, topk_idx = torch.topk(logits, k=min(top_k, logits.shape[-1]))\n",
    "    #             mask = torch.full_like(logits, float('-inf'))\n",
    "    #             mask.scatter_(1, topk_idx, topk_vals)\n",
    "    #             logits = mask\n",
    "    #         probs = F.softmax(logits, dim=-1)\n",
    "    #         next_token = torch.multinomial(probs, num_samples=1)\n",
    "    #         idx = torch.cat([idx, next_token], dim=1)\n",
    "    #     return idx\n",
    "\n",
    "    # generation for bpe tokenizer\n",
    "    # @torch.no_grad()\n",
    "    # def generate(model, idx, max_new_tokens=50, temperature=1.0, top_k=None):\n",
    "    #     for _ in range(max_new_tokens):\n",
    "    #         logits, _ = model(idx)\n",
    "    #         logits = logits[:, -1, :] / temperature\n",
    "    #         if top_k is not None:\n",
    "    #             v, ix = torch.topk(logits, top_k)\n",
    "    #             logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "    #         probs = torch.softmax(logits, dim=-1)\n",
    "    #         next_id = torch.multinomial(probs, num_samples=1)\n",
    "    #         idx = torch.cat([idx, next_id], dim=1)\n",
    "    #         if next_id.item() == tokenizer.token_to_id(\"<eos>\"):\n",
    "    #             break\n",
    "    #     return idx\n",
    "\n",
    "    # generation for sentencepiece\n",
    "    @torch.no_grad()\n",
    "    def generate(model, idx, max_new_tokens=50, temperature=1.0, top_k=None):\n",
    "        eos_id = sp.eos_id()  # SentencePiece special token\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = model(idx)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, ix = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "            # STOP when EOS token appears\n",
    "            if next_id.item() == eos_id:\n",
    "                break\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f5b7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a378351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMS:\n",
      "        d_model = 512\n",
      "        num_layers = 6\n",
      "        num_heads = 8\n",
      "        learning_rate = 0.0001\n",
      "        dropout = 0.0\n",
      "Epoch 1: train_loss=6.068 | val_loss=5.104 | val_ppl=164.6\n",
      "Epoch 2: train_loss=5.302 | val_loss=4.820 | val_ppl=124.0\n",
      "Epoch 3: train_loss=4.923 | val_loss=4.651 | val_ppl=104.7\n",
      "Epoch 4: train_loss=4.617 | val_loss=4.546 | val_ppl=94.2\n",
      "Epoch 5: train_loss=4.345 | val_loss=4.485 | val_ppl=88.7\n",
      "Epoch 6: train_loss=4.087 | val_loss=4.462 | val_ppl=86.7\n",
      "Epoch 7: train_loss=3.832 | val_loss=4.478 | val_ppl=88.1\n",
      "Epoch 8: train_loss=3.572 | val_loss=4.529 | val_ppl=92.7\n",
      "Epoch 9: train_loss=3.304 | val_loss=4.609 | val_ppl=100.4\n",
      "Epoch 10: train_loss=3.024 | val_loss=4.733 | val_ppl=113.6\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# model = MiniGPT(vocab_size=tokenizer.get_vocab_size(), d_model=d_model,\n",
    "#                 num_layers=num_layers, num_heads=num_heads,\n",
    "#                 block_size=block_size).to(DEVICE)\n",
    "model = MiniGPT(vocab_size=sp.get_piece_size(), d_model=d_model,\n",
    "                num_layers=num_layers, num_heads=num_heads,\n",
    "                block_size=block_size).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "\n",
    "def compute_loss(logits, targets):\n",
    "    # shift for next-token prediction\n",
    "    return F.cross_entropy(\n",
    "        logits[:, :-1, :].reshape(-1, logits.size(-1)),\n",
    "        targets[:, 1:].reshape(-1)\n",
    "    )\n",
    "\n",
    "\n",
    "def run_epoch(loader, train: bool = True):\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    for (batch,) in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits, _ = model(batch)\n",
    "            loss = compute_loss(logits, batch)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "print(f'PARAMS:\\n\\\n",
    "        d_model = {d_model}\\n\\\n",
    "        num_layers = {num_layers}\\n\\\n",
    "        num_heads = {num_heads}\\n\\\n",
    "        learning_rate = {learning_rate}\\n\\\n",
    "        dropout = {dropout}')\n",
    "history = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    history.append((train_loss, val_loss, val_ppl))\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.3f} | val_loss={val_loss:.3f} | val_ppl={val_ppl:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff10d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt tokenizer\n",
    "\n",
    "# prompt = \"The history of natural language processing\"\n",
    "# input_ids = tokenizer(prompt, return_tensors='pt')[\"input_ids\"].to(DEVICE)\n",
    "# with torch.no_grad():\n",
    "#     generated_ids = model.generate(input_ids, max_new_tokens=50, temperature=0.9, top_k=50)\n",
    "# print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history of natural language processing , and in February 2008 . She was the first of the  (  ) , who played five of the ten @-@ year @-@ old  . She performed for the song on The A Rush of The New York Times Square , as well\n"
     ]
    }
   ],
   "source": [
    "# bytelevelbpe tokenizer\n",
    "\n",
    "# prompt = \"The history of natural language processing\"\n",
    "\n",
    "# bos_id = tokenizer.token_to_id(\"<bos>\")\n",
    "# prompt_ids = [bos_id] + tokenizer.encode(prompt).ids\n",
    "# input_ids = torch.tensor([prompt_ids], device=DEVICE)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     generated_ids = model.generate(\n",
    "#         input_ids,\n",
    "#         max_new_tokens=50,\n",
    "#         temperature=0.9,\n",
    "#         top_k=50\n",
    "#     )\n",
    "# print(\n",
    "#     tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "264e4297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history of natural language processing in the history of the tale was widely admired ; and , by the time the death of her husband , <unk> her son , was not later influenced by <unk> , who was <unk> to have been the first female victim by the\n"
     ]
    }
   ],
   "source": [
    "# sentencepiece tokenizer\n",
    "\n",
    "prompt = \"The history of natural language processing\"\n",
    "\n",
    "bos_id = sp.bos_id()   # <-- SentencePiece BOS\n",
    "prompt_ids = [bos_id] + sp.encode(prompt, out_type=int)\n",
    "\n",
    "input_ids = torch.tensor([prompt_ids], device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.9,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "# SentencePiece decode\n",
    "print(sp.decode(generated_ids[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
